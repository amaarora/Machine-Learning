{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Congenital Disorder using Weighted Random Forest\n",
    "https://www.kaggle.com/c/ga-dat-syd13/data\n",
    "\n",
    "Author: <div class=\"LI-profile-badge\"  data-version=\"v1\" data-size=\"medium\" data-locale=\"en_US\" data-type=\"horizontal\" data-theme=\"dark\" data-vanity=\"aroraaman\"><a class=\"LI-simple-link\" href='https://au.linkedin.com/in/aroraaman?trk=profile-badge'>Aman Arora</a></div>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**So, What is a congenital disorder?** <br> \n",
    "Most babies are born healthy, but when a baby has a condition that is present from birth, it is called a congenital disorder. Congenital disorders can be inherited or caused by environmental factors and their impact on a childâ€™s health and development can vary from mild to severe. A child with a congenital disorder may experience a disability or health problems throughout life. (https://www.pregnancybirthbaby.org.au/what-is-a-congenital-disorder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here are some of the resources that I referenced before creating this notebook:**<br>\n",
    "1. https://www.kaggle.com/pmarcelino/comprehensive-data-exploration-with-python \n",
    "2. General Assembly, Sydney - Linear Regression iPython Notebook -\n",
    "Authors: Kevin Markham (Washington, D.C.), Ed Podojil (New York City); <br>\n",
    "Taught by: **Dima Galat** (https://www.linkedin.com/in/dimagalat/)\n",
    "5. https://www.booktopia.com.au/multivariate-data-analysis-joe-f-hair/prod9781292021904.html?source=pla&gclid=EAIaIQobChMIpJ2qkJLO3QIV16mWCh3RBAFUEAQYASABEgL39vD_BwE (MultiVariate Data Analysis)\n",
    "6. https://www-bcf.usc.edu/~gareth/ISL/ Introduction to Statistical Learning (James et al., 2014)\n",
    "7. https://www.kaggle.com/erikbruin/house-prices-lasso-xgboost-and-a-detailed-eda\n",
    "8. https://www.kaggle.com/juliencs/a-study-on-regression-applied-to-the-ames-dataset\n",
    "9. https://www.kaggle.com/apapiu/regularized-linear-models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the toolkit together\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt \n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.svm import SVC \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split, KFold\n",
    "from sklearn.metrics import confusion_matrix,recall_score,precision_recall_curve,auc,\\\n",
    "                            roc_curve,roc_auc_score,classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn import svm\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I performed initial EDA for this data set in my notebook 'Predicting Congenital Disorder using Resampling.ipynb'. Therefore, here I will just clean my data - remove nulls and implement Support Vector Machine model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing data\n",
    "train_path = '/Users/user/Desktop/Folders/Data_Scientist/Project 3_ GA/health-diagnostics-train.csv'\n",
    "health_df =pd.read_csv(train_path)\n",
    "\n",
    "# importing data\n",
    "test_path = '/Users/user/Desktop/Folders/Data_Scientist/Project 3_ GA/health-diagnostics-test.csv'\n",
    "health_df_test =pd.read_csv(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "health_df.replace('#NULL!', np.NaN, inplace = True)\n",
    "health_df_test.replace('#NULL!', np.NaN, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have tried different combinations of imputing data and dropping data already while working on this exercise. It makes sense to impute the mode() in missing values as these are categorical features. However, imputing in test with mode() and dropping train values gives maximum accuracy. <br>\n",
    "Why? Well, first of all we have over 30,000 values in train data. Losing 900 is approx 3% of the data. This data is not unique and exists for target == 0, that is, our majority class. We are not missing the minority class, therefore, it is safe to drop this data in Train. For our test data, we cannot drop any data and therefore, we will be imputing with mode(). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "health_df.dropna(inplace = True)\n",
    "health_df_test = health_df_test.apply(lambda x:x.fillna(x.value_counts().index[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TRAIN TEST SPLIT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = health_df.loc[:, health_df.columns != 'target']\n",
    "y = health_df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2, random_state = 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24348, 9)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3630, 9)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Weighted Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0: 24299\n",
      "Class 1: 49\n",
      "Proportion of majority to minority class: 495.9 : 1\n"
     ]
    }
   ],
   "source": [
    "target_count = pd.Series(y_train).value_counts()\n",
    "\n",
    "print('Class 0:', target_count[0])\n",
    "print('Class 1:', target_count[1])\n",
    "print('Proportion of majority to minority class:', round(target_count[0] / target_count[1], 2), ': 1')\n",
    "\n",
    "prop = round(target_count[0] / target_count[1],2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I ran random search followed by Grid Search to find the best class weight. It turns the best class weight was the inverse of proportion of class in our Dataset. It is also suggested in the research article \"Predicting congenital heart defects: A comparison of three data mining methods\" by Yanhong Luo. (https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0177811)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the recall for this model is : 0.03636363636363636\n",
      "TP 132\n",
      "TN 0\n",
      "FP 0\n",
      "FN 3498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:22: RuntimeWarning: invalid value encountered in long_scalars\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Only one class present in y_true. ROC AUC score is not defined in that case.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-0b7a6fc14585>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"FP\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mFP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"FN\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mFN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"AUC: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'PRECISION:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'TPR:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mTPR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/metrics/ranking.py\u001b[0m in \u001b[0;36mroc_auc_score\u001b[0;34m(y_true, y_score, average, sample_weight)\u001b[0m\n\u001b[1;32m    275\u001b[0m     return _average_binary_score(\n\u001b[1;32m    276\u001b[0m         \u001b[0m_binary_roc_auc_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m         sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/metrics/base.py\u001b[0m in \u001b[0;36m_average_binary_score\u001b[0;34m(binary_metric, y_true, y_score, average, sample_weight)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbinary_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/metrics/ranking.py\u001b[0m in \u001b[0;36m_binary_roc_auc_score\u001b[0;34m(y_true, y_score, sample_weight)\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_binary_roc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m             raise ValueError(\"Only one class present in y_true. ROC AUC score \"\n\u001b[0m\u001b[1;32m    269\u001b[0m                              \"is not defined in that case.\")\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Only one class present in y_true. ROC AUC score is not defined in that case."
     ]
    }
   ],
   "source": [
    "RF = RandomForestClassifier(n_estimators = 500,\n",
    "                            class_weight={0: 1/prop,1:1.6},\n",
    "                            max_features = 3,\n",
    "                            min_samples_leaf = 65,\n",
    "                            criterion = 'gini',\n",
    "                            random_state = 123,\n",
    "                            bootstrap = True\n",
    "                           )\n",
    "RF.fit(X_train,y_train)\n",
    "\n",
    "y_pred = RF.predict(X_test)\n",
    "\n",
    "#Evaluation\n",
    "cnf_matrix=confusion_matrix(y_test,y_pred)\n",
    "TP = cnf_matrix[1,1,]\n",
    "TN = cnf_matrix[0,0]\n",
    "FP = cnf_matrix[0,1]\n",
    "FN = cnf_matrix[1,0]\n",
    "TPR = TP/(TP+FN)\n",
    "TNR = TN/(TN+FP)\n",
    "ACC = (TP+TN)/(TP+TN+FP+FN)\n",
    "wtACC = (0.7*TPR) + (0.3*TNR)\n",
    "prec = TP/(TP+FP)\n",
    "G_mean = np.sqrt(TPR * TNR)\n",
    "print(\"the recall for this model is :\",cnf_matrix[1,1]/(cnf_matrix[1,1]+cnf_matrix[1,0]))\n",
    "print(\"TP\",TP) \n",
    "print(\"TN\",TN) \n",
    "print(\"FP\",FP) \n",
    "print(\"FN\",FN) \n",
    "print(\"AUC: {}\".format(roc_auc_score(y_test, y_pred)))\n",
    "print('PRECISION:',prec)\n",
    "print('TPR:',TPR)\n",
    "print('TNR:',TNR)\n",
    "print('ACC:',ACC)\n",
    "print('wtACC:',wtACC)\n",
    "print('G_mean:',G_mean)\n",
    "print('-'*100)\n",
    "\n",
    "fpr, tpr, threshold = roc_curve(y_test, y_pred)\n",
    "auc_score = roc_auc_score(y_test, y_pred)\n",
    "print('AUC score:', auc_score, 'for model:',RF)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='red',\n",
    "lw=2, label='ROC curve')\n",
    "plt.plot([0, 1], [0, 1], color='blue', lw=2, linestyle='--')\n",
    "##Title and label\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HYPERPARAMETER TUNING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Class Weight \n",
    "# class_weight=[{0: 1/prop, 1: w} for w in np.linspace(start = 1, stop = 3, num = 20)]\n",
    "\n",
    "# # Number of trees in random forest\n",
    "# n_estimators = [100,200,300,400,500]\n",
    "\n",
    "# # Number of features to consider at every split\n",
    "# max_features = [2,3,4,5,6,7,8,9]\n",
    "\n",
    "# # Maximum number of levels in tree\n",
    "# max_depth = [int(x) for x in np.linspace(10, 1000, num = 50)]\n",
    "# max_depth.append(None)\n",
    "# # Minimum number of samples required to split a node\n",
    "\n",
    "# # Minimum number of samples required at each leaf node\n",
    "# min_samples_leaf = [45,50,55,60,65,70,75,80,100,150,200]\n",
    "# # Method of selecting samples for training each tree\n",
    "# bootstrap = [True, False]\n",
    "# #criterion - measure of quality of split \n",
    "# criterion = ['gini','entropy']\n",
    "# # Create the random grid\n",
    "# random_grid = {'n_estimators': n_estimators,\n",
    "#                'class_weight':class_weight,\n",
    "#                'max_features': max_features,\n",
    "#                'max_depth': max_depth,\n",
    "#                'min_samples_leaf': min_samples_leaf,\n",
    "#                'bootstrap': bootstrap,\n",
    "#                'criterion':criterion}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use the random grid to search for best hyperparameters\n",
    "\n",
    "# rf = RandomForestClassifier()\n",
    "\n",
    "# rf_random = RandomizedSearchCV(estimator = rf, \n",
    "#                                param_distributions = random_grid, \n",
    "#                                n_iter = 200, \n",
    "#                                cv = 3, \n",
    "#                                scoring = 'roc_auc',\n",
    "#                                verbose=2, \n",
    "#                                random_state=None, \n",
    "#                                n_jobs = -1)\n",
    "\n",
    "# # Fit the random search model\n",
    "# rf_random.fit(X_train,y_train)\n",
    "\n",
    "# # # RUNTIME ~35mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf_random.best_estimator_\n",
    "# # >>> RandomForestClassifier(bootstrap=True,\n",
    "# #             class_weight={0: 0.0018015096650993532, 1: 1.1052631578947367},\n",
    "# #             criterion='entropy', max_depth=460, max_features=9,\n",
    "# #             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
    "# #             min_impurity_split=None, min_samples_leaf=70,\n",
    "# #             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "# #             n_estimators=100, n_jobs=1, oob_score=False, random_state=None,\n",
    "# #             verbose=0, warm_start=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
